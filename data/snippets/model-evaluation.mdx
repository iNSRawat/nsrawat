---
title: Model Evaluation Metrics
description: 'Comprehensive model evaluation with classification report, confusion matrix, and visualization'
date: 2024-12-08
tags:
  - machine-learning
  - evaluation
  - python
  - sklearn
---

## Model Evaluation Metrics

A comprehensive function to evaluate classification models with detailed metrics and visualizations.

```python
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

def evaluate_model(y_true, y_pred, y_proba=None, labels=None, figsize=(12, 5)):
    """
    Comprehensive model evaluation with metrics and visualizations.

    Args:
        y_true: True labels
        y_pred: Predicted labels
        y_proba: Predicted probabilities (optional, for ROC curve)
        labels: Class labels
        figsize: Figure size for plots
    """
    # Classification Report
    print("=" * 60)
    print("CLASSIFICATION REPORT")
    print("=" * 60)
    print(classification_report(y_true, y_pred, target_names=labels))

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)

    # Plot Confusion Matrix
    plt.figure(figsize=figsize)

    plt.subplot(1, 2, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')

    # ROC Curve (if probabilities provided)
    if y_proba is not None:
        plt.subplot(1, 2, 2)
        fpr, tpr, _ = roc_curve(y_true, y_proba)
        auc_score = roc_auc_score(y_true, y_proba)
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return {
        'confusion_matrix': cm,
        'classification_report': classification_report(y_true, y_pred,
                                                       target_names=labels,
                                                       output_dict=True)
    }

# Usage Example
# evaluate_model(y_test, y_pred, y_proba=y_pred_proba,
#                labels=['Class 0', 'Class 1'])
```
